# Web Scraper to SQLite DB - Production Configuration
# =====================================================

# ====================
# SCRAPING CONFIGURATION
# ====================
scraping:
  # Primary data source (can be overridden via CLI)
  target_url: "file://./data/input/example.html"  # Use local file for reliable testing
  
  # Alternative sources (used if primary fails)
  fallback_urls:
    - "file://./data/input/backup_data.html"
    - "https://web.archive.org/web/20231015000000/https://example-store.com/products"
  
  # HTTP Client Configuration
  user_agent: "Mozilla/5.0 (compatible; Data-Analyst-Portfolio-Scraper/1.0; +https://github.com/yourusername/portfolio)"
  timeout: 30  # seconds
  retry_attempts: 3
  rate_limit: 10  # requests per minute
  delay_between_requests: 0.1  # seconds
  
  # Headers for request simulation
  headers:
    Accept-Language: "en-US,en;q=0.9"
    Accept-Encoding: "gzip, deflate, br"
    Connection: "keep-alive"
    Referer: "https://www.google.com/"
    DNT: "1"  # Do Not Track
    
  # Respectful scraping settings
  respect_robots_txt: true
  max_pages_per_domain: 100
  concurrent_requests: 5
  
  # Cache settings
  cache_enabled: true
  cache_ttl: 3600  # seconds (1 hour)
  cache_dir: "data/cache"
  
  # Monitoring and alerts
  enable_metrics: true
  alert_on_failure: true
  success_threshold: 0.8  # 80% success rate

# ====================
# DATA SELECTORS
# ====================
selectors:
  # Container selector (CSS selector for product containers)
  container: "div.product-item, li.product, article.item, .product-card"
  
  # Field extraction selectors
  # Format: field_name: "css_selector::extraction_type"
  # Extraction types: ::text, ::datetime, ::attr(attribute_name)
  fields:
    id: ".product-id::text"
    title: "h2.product-title::text, h3.title::text, .name::text"
    price: ".price::text, .amount::text, [itemprop='price']::attr(content)"
    original_price: ".original-price::text, .compare-at-price::text"
    description: ".description::text, .product-desc p::text, [itemprop='description']::attr(content)"
    category: ".category::text, .breadcrumb li:nth-child(2)::text"
    brand: ".brand::text, [itemprop='brand']::text"
    rating: ".rating::attr(data-score), .stars::attr(title)"
    review_count: ".review-count::text, .reviews::text"
    stock_status: ".stock::text, .availability::text, [itemprop='availability']::attr(href)"
    sku: ".sku::text, [itemprop='sku']::text"
    url: "a::attr(href)"  # Relative URLs will be made absolute
    image_url: "img.product-image::attr(src), [itemprop='image']::attr(src)"
    date_published: "time::datetime, [itemprop='datePublished']::attr(content)"
    tags: ".tags::text, .labels::text"
    
  # Dynamic selector strategies (for AJAX sites)
  dynamic_selectors:
    json_ld: true  # Extract structured data from JSON-LD
    microdata: true  # Extract microdata
    open_graph: true  # Extract Open Graph tags
    
  # Pagination selector (for multi-page scraping)
  pagination:
    next_page: "a.next::attr(href), .pagination-next::attr(href)"
    max_pages: 10

# ====================
# DATABASE CONFIGURATION
# ====================
database:
  # SQLite database path
  db_path: "data/scraped_data.db"
  
  # Table configuration
  table_name: "scraped_records"
  
  # Schema definition (auto-detected if not specified)
  schema:
    id: "INTEGER PRIMARY KEY AUTOINCREMENT"
    title: "TEXT NOT NULL"
    price: "REAL"
    original_price: "REAL"
    description: "TEXT"
    category: "TEXT"
    brand: "TEXT"
    rating: "REAL"
    review_count: "INTEGER"
    stock_status: "TEXT"
    sku: "TEXT"
    url: "TEXT UNIQUE"
    image_url: "TEXT"
    date_published: "TEXT"
    tags: "TEXT"
    source_url: "TEXT"
    scraped_at: "TIMESTAMP DEFAULT CURRENT_TIMESTAMP"
    updated_at: "TIMESTAMP DEFAULT CURRENT_TIMESTAMP"
    
  # Indexes for performance
  indexes:
    - "idx_category (category)"
    - "idx_price (price)"
    - "idx_rating (rating)"
    - "idx_stock_status (stock_status)"
    - "idx_scraped_at (scraped_at)"
    
  # Performance settings
  batch_size: 100  # Records per insert batch
  journal_mode: "WAL"  # Write-Ahead Logging for concurrency
  cache_size: -2000  # 2MB cache in KB (negative means in pages)
  
  # Backup settings
  backup_enabled: true
  backup_dir: "data/backups"
  backup_retention_days: 7

# ====================
# DATA CLEANING CONFIGURATION
# ====================
cleaning:
  # Date formatting
  date_format: "%Y-%m-%d"
  datetime_format: "%Y-%m-%d %H:%M:%S"
  
  # Column type definitions
  numeric_columns: ["price", "original_price", "rating", "review_count"]
  text_columns: ["title", "description", "category", "brand", "stock_status", "sku", "tags"]
  date_columns: ["date_published", "scraped_at", "updated_at"]
  url_columns: ["url", "image_url", "source_url"]
  
  # Missing value handling
  missing_value_strategies:
    numeric: "median"  # mean, median, mode, or specific value
    text: "Not Specified"
    date: "NULL"
    url: "NULL"
    
  # Outlier detection
  outlier_detection:
    enabled: true
    method: "iqr"  # iqr, zscore, or percentile
    threshold: 1.5  # IQR multiplier
    
  # Text normalization
  text_normalization:
    trim_whitespace: true
    remove_extra_spaces: true
    lowercase: false  # Keep original case for titles
    remove_special_chars: false  # Keep special chars for SKUs, etc.
    
  # Price normalization
  price_cleaning:
    currency_symbols: ["$", "€", "£", "¥", "₹"]
    remove_currency: true
    decimal_separator: "."
    thousand_separator: ","
    
  # Duplicate handling
  drop_duplicates: true
  duplicate_columns: ["title", "sku"]  # Columns to check for duplicates
  keep_duplicate: "first"  # first, last, or none
  
  # Data validation rules
  validation_rules:
    price:
      min: 0
      max: 10000
      required: false
    rating:
      min: 0
      max: 5
      required: false
    title:
      min_length: 2
      max_length: 500
      required: true

# ====================
# ANALYTICS & REPORTING
# ====================
analytics:
  # Data quality metrics
  quality_metrics:
    completeness_threshold: 0.8  # 80% required fields filled
    accuracy_threshold: 0.9  # 90% data accuracy
    timeliness_hours: 24  # Data should be less than 24 hours old
    
  # Business metrics to calculate
  business_metrics:
    - "avg_price_by_category"
    - "total_products_by_brand"
    - "out_of_stock_percentage"
    - "price_distribution"
    - "rating_distribution"
    
  # Export formats
  export_formats:
    csv: true
    json: true
    excel: true
    parquet: true
    
  # Dashboard integration
  dashboard:
    powerbi_connection_string: ""
    tableau_connection_string: ""
    refresh_interval: 3600  # seconds

# ====================
# LOGGING CONFIGURATION
# ====================
logging:
  # Log levels
  level: "INFO"  # DEBUG, INFO, WARNING, ERROR, CRITICAL
  file: "logs/scraper.log"
  
  # Log rotation
  max_file_size: 10485760  # 10MB
  backup_count: 5
  encoding: "utf-8"
  
  # Log format
  format: "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
  date_format: "%Y-%m-%d %H:%M:%S"
  
  # Component-specific logging
  components:
    scraper: "INFO"
    cleaner: "INFO"
    database: "WARNING"
    cli: "INFO"

# ====================
# PERFORMANCE SETTINGS
# ====================
performance:
  # Memory management
  max_memory_mb: 512
  chunk_size: 1000  # Process records in chunks
  
  # Parallel processing
  max_workers: 4
  use_multiprocessing: true
  
  # Timeouts
  extraction_timeout: 300  # 5 minutes
  cleaning_timeout: 60  # 1 minute
  database_timeout: 30  # 30 seconds

# ====================
# SECURITY SETTINGS
# ====================
security:
  # Data protection
  encrypt_database: false
  encrypt_cache: false
  
  # Privacy compliance
  anonymize_data: false
  gdpr_compliant: true
  remove_pii: true
  
  # Access control
  require_authentication: false
  api_key: ""
  
  # Network security
  use_proxy: false
  proxy_list: []
  verify_ssl: true

# ====================
# MONITORING & ALERTS
# ====================
monitoring:
  # Health checks
  health_check_interval: 300  # 5 minutes
  
  # Performance metrics collection
  collect_metrics: true
  metrics_port: 9090
  
  # Alerting
  alerts:
    email_enabled: false
    email_recipients: []
    slack_enabled: false
    slack_webhook: ""
    
  # Thresholds for alerts
  thresholds:
    error_rate: 0.05  # 5%
    extraction_time: 60  # seconds
    memory_usage: 0.8  # 80%
    disk_usage: 0.9  # 90%

# ====================
# ENVIRONMENT SETTINGS
# ====================
environment:
  # Deployment environment
  mode: "development"  # development, staging, production
  
  # Feature flags
  features:
    async_scraping: true
    cache_warming: false
    incremental_updates: true
    data_validation: true
    
  # Environment variables (override in production)
  env_vars:
    DATABASE_URL: "sqlite:///data/scraped_data.db"
    LOG_LEVEL: "INFO"
    DEBUG: "false"

# ====================
# EXAMPLE CONFIGURATIONS
# ====================
examples:
  # E-commerce site example
  ecommerce:
    container: "div.product"
    fields:
      title: "h2::text"
      price: ".price::text"
      description: ".desc::text"
  
  # News site example  
  news:
    container: "article"
    fields:
      title: "h1::text"
      content: ".article-body::text"
      date: "time::datetime"
      author: ".author::text"
  
  # Real estate example
  real_estate:
    container: ".listing"
    fields:
      address: ".address::text"
      price: ".price::text"
      bedrooms: ".beds::text"
      bathrooms: ".baths::text"
      square_feet: ".sqft::text"